# SISTEMI OPERATIVI - 02

## Ripasso di Architettura

Architettura di calcolatore divisa in
->CPU
->Disk controller
->Usb controller
->Graphic Adapter

Connessi da un bus con la memoria

-->Hardware progettato assieme al SO per far funzionare tutto al meglio

### IL PROCESSORE

CPU come unità capace di eseguire istruzioni (logiche, aritmetiche, di controllo.). Tutto gestito attraverso il ciclo FETCH/DECODE/EXECUTE della CPU.
Le CPU moderne sono più complesse per ottimizzare il ciclo stesso
Interno  CPU: SET DI REGISTRI, hanno il compito di ospitare durante la computazione alcuni dati utili alle istruzioni.<br>
Molti sono considerati generici, alcuni sono specializzati (Program Counter (PC), Stack Pointer (SP), Program status word(PSW))<br>
    -> **PC** ha il compito di identificare la prossima istruzione --> Indirizzo depositato in questo registro della prossima istruzione da eseguire. Alcune istruzioni sono lineari tra di loro quindi PC incrementa, altre invece hanno bisogno di salti<br><br>
    -> **SP** -> indica la testa dello stack. Struttura dati. E' una pila (LIFO) e contiene dati, words, ma possiamo utilizzarlo e vederlo come una pila di frames di attivazione. Esse rappresentano la zona di memoria dove è conservato il contenuto di una procedura/funzione (linguaggi modulari, ma funziona in genere così). Salvato al suo interno informazioni utili per tornare dove si era prima --> viene salvato il Program Counter. Il RETURN preleva il dato dallo STACK e permette di tornare all'informazione dov'era prima di entrare nella chiamata. Il frame di attivazione può contenere anche altre cose, ad esempio una funzione può avere variabili-> Come vengono passati? Utilizzando i registri -> ci si mette d'accordo anche guardando quali registri sono liberi etc. Nel Frame di attivazione ci può stare anche altro. -> E con le ricorsive?? Nel discorso ricorsivo ogni volta che chiamo ricorsivamente, viene creato un frame di attivazione per ogni volta che A viene chiamata. Sperando che il loop finisca altrimenti so cazzi.
    Nel considerare una istantanea di che sta facendo qualcosa in quel momento si deve considerare anche lo stack = è uno storico di quello che sta succedendo. L'SP punta sempre alla testa dello stack. Quando "Si torna" nel main dopo la ricorsione, gli spazi di attivazione delle chiamate ricorsive vengono eliminati. Dato che sia lo stack che i dati sono variabili e dinamici, per implementare stack e dati, lo stack viene messo alla fine dello spazio disponibile. Lo stack viene capovolto e lo SP punta alla testa, è come se crescesse dalla fine verso l'inizio. I dati crescono nella direzione opposta con spazio in mezzo. Così funziona lo spazio di indirizzamento di ogni processo.

![stack](media/so1.png)

    ->**PSW** è speciale, è più una collezione di informazioni (flag, bits) e storano stati di informazione di un  programma. Flags legati alle istruzioni di controllo. Ad esempio in assembly, per un jump condizionato -> le istruzioni di confronto vanno a depositare nella PSW "com'è andato" il confronto. Le compares depositano l'esito della compare in alcuni degli spazi del PSW. Le istruzioni di confronto vanno a depositare l'esito e una jump condizionato quindi controlla quel dato e decide se saltare o non saltare. Nelle CPU con diverse modalità --> C'è un bit preciso che dice proprio questa informazione, se è in kernel mode o in usermode. Se una istruzione non ha i permessi per eseguire (0 nel bit mode) allora c'è una ECCEZIONE e il controllo passa al sistema operativo. Può essere un bug o simile, e in quel caso il codice viene interrotto (processo terminato). La CPU passa il controllo al SO. L'SO sa come procedere. Anche un semplice FETCH se va a prendere una informazione che non può usare -> poi viene annullata. Ma il fatto che venga prelevata creava problemi di sicurezza.
    Il bit "k" (quello della mode) è speciale perché la CPU non è libera di modificarlo quando vuole. Viene modificato da processi particolari. Questi registri nei contesti di CONTEXT SWITCH --> Nel parallelismo fittizio dei processi (cpu virtuali) --> bisogna salvare lo stato della CPU al momento. Se ne occupa l'SO. 

-> Nelle architetture reali in realtà utilizzano strutture più articolate ovvero pipelines e strutture superscalari. Permettono di avere unità all'interno della CPU specializzate in azioni (Fetch Decode Execute) che quindi possono andare in  parallelo. Così ognuna lavora a pieno regime (Fetch n+2, Decode n+1, Execute n; 3 informazioni.) Questo meccanismo deve venire aggiustato per i salti, ma ci sta. Ulteriore generalizzazione è nelle superscalari --> Hanno anche unità di esecuzione specializzate che vanno in parallelo; però si creano problemi. Parallelismo cozza con la sequenzialità delle informazioni. Questo permette di eseguire informazioni fuori ordine ma solo quando questo non crea casini. In alcuni casi alcuni effetti devono addirittuara essere annullate.

---

### Modalità di esecuzione

Il SO deve fare da tramite -> Tutto viene gestito dalle chiamate di sistema. Syscalls. 
    E' un  servizio/porta di  accesso del sistema operativo. Invocheranno una o più chiamata di sistema che permettono di operare sul kernelmode. Servizi che permettono di aprire un oggetto, modificarlo, chiuderlo. SOno tutte chiamate di sistema. Il processo utente non ha mai messo direttamente mano al disco stesso. Ho un processo utente il cui codice fa syscalls.

-->TRAP: è una call speciale. Indica a una istruzione    jdsjfh
    Implica una commutazione da usermode a kernelmode. Come effetto della invocazione della trap (Salto fuori dal processo in cui sono perché essa permette di identificare la syscall che mi serve) il bit K passa a 1, e la parte che serve, la syscall viene eseguita in modalità kernel. Quel codice viene scritto dai developers del kernel stesso (CI FIDIAMO, hellooo Linus Torvalds) alla fine della esecuzione della syscall c'è una return che farà il contrario della trap, e il return cambia il bit k  di modalità di nuovo a 0. Nessun pezzo del codice del processo viene runnato in modalità kernel, ma l'SO permette di fare syscalls specifiche. La trap fa anche qualcosa di molto simile a quello che fa la call: salva nello stack ciò che viene sporcato facendo il salto (il PC) e anche la PSW nella sua interezza; così ripristina anche il bit K a 0 dopo.

-->Tutte le syscalls fanno operazioni di routine comuni che salvano tutti i registri per intero nello stack, perché li potrebbe utilizzare. Alla return, ripristina anche i vecchi registri in modo che i valori dopo la syscalls siano ripristinati.

--> Il sistema operativo in realtà ha il proprio stack-> uno stack specifico al kernel. 

[ --------- ]

--> La tabella di dove andare a prendere le syscalls è salvata nel kernel. Due tabelle diverse, una nel processore una nel kernel. Istruzione <-> Numero nel processore, 
Numero <-> Indirizzo memoria nel kernel
  
### INTERRUPT

Possono essere legati ad eventi oppure ad input oppure a clock di sistema. Ciò che sta facendo la cpu in quel momento viene interrotto. C'è una tabella che per ogni interrupt mi dice dove sta la relativa funzione. Ogni volta che si manifesta un interrupt si manifesta la routine allegata. E deve funzionare a prescindere da cosa sta facendo la CPU. Essa potrebbe in quel momento eseguire un processo, oppure potrebbe stare eseguendo una parte del kernel. Non crea problemi, di solito. Tuttavia interrompere il kernel in momenti delicati può essere rischioso. Quindi il kernel in alcuni momenti critici disattiva il sistema di notifica. Ma deve ripristinarlo al più presto. Dimenticarsi di riattivare gli interrupts è un disastro. 

Cosa succede quando avviene un interrupt? Qualcosa di simile a quello che avviene dopo una trap. Permette di salvare le cose che vengono interrotte. Vengono salvate tutte cose. Preleva il PC, salva tutti gli altri registri -> il controllo passa alla routine collegata all'interrupt corrispondente. L'interrupt fa quello che deve fare. (L'interrupt deve commutare in modalità kernel. Il codice interrupt deve lavorare con i pieni poteri, è prerogativa del sistema operativo decidere cosa deve fare.) Poi ripristina tutto com'era prima


---

## Più processori

Multithreading 
-> Tiene all'interno della CPU lo stato di due threads

Stalli per la cpu e i tempi morti -> idea di includere dentro la CPU un altro set di registri (Sempre con una sola unità di esecuzione). Avendo due set di registri -> Ci sono due contesti. Lasciamo che la CPU possa commutare da un contesto a un altro in maniera estremamente rapida a livello di hardware. Non c'è nessun parallelismo: non li utilizza contemporaneamente, è un timesharing che viene fatto in tempi morti. --> Si parla di THREAD e non di processi differenti. E' un unico processo. Un processo al suo interno ha più flussi di esecuzione. Quindi si parla di questi threads. L'idea è di battere i tempi morti eseguendo codice sempre e rapidamente.

Queste architetture trattano questo tipo di CPU come se fossero 2 CPU. Il sistema operativo deve avere cognizione e distinguere come funzioni l'hardware sfruttandolo al meglio.

In generale aumenta la produttività del sistema che cerca di misurare quanto lavoro riesce a macinare il sistema in un T. A parte la CPU il resto è condiviso -> ragioni economiche.

I processi possono comunicare tramite la ram. Interprocess communication >>>>> Networking

Perché farli comunicare? esempio: compilare kernel linux  -> utilizzare più cpu, svolgere stesso compito più rapidamente se in pseudo-parallelo.

Vantaggi economici, efficienza, ma anche per --> Passando da 1 cpu a N CPU vuol dire che ci aspettiamo un miglioramento di fattore N? Non per forza. Condividere alcune risorse e lavorare sugli stessi dati a volte è un ostacolo: per evitare problemi (raise? condition), quando i dati sono errati perché si pasticcia con altri processi con gli stessi dati, allora i processi si fermano per fare aspettare turni. Sono pause e tempi morti, però sono strettamente necessari per garantire la correttezza delle operazioni. Però si guadagna moltissimo comunque quindi ok

### Multicore

Quando i core sono all'interno dello stesso chip. Ad esempio schede madri con più zoccoli CPU, però ogni singoli cpu può avere più core: dal punto di vista operativo non cambia molto, è una questione di progettazione e di velocità. Lavorare all'interno dello stesso package permette di ridurre comunicazione e creano meno calore. Per questo sono usati nei mobile. 
Problema di sparare la CPU al massimo è che si fa più calore e diventa più instabile se si alza la frequenza concentrando tutto in uno. 

---

## MEMORIE

Gerarchia-> dai più efficienti e veloci e meno capienti e più costose.

Registri <-> Cache <-> Memoria Centrale <-> Electronic Disk <-> Magnetic Disk <-> Optical disk

RAM direttamente gestita dal SO.  La CPU non può accedere direttamente a qualcosa che sta su disco. Ha bisogno di trasferire le cose nella RAM. Si può lavorare con file grandissimi caricando pochi mb alla volta.

Limite della RAM: è volatile. Sotto la memoria centrale c'è quella non volatile. Però serve.

Caso della cache --> Ce ne possono essere diversi tipi e si frappone tra la CPU e la RAM cercando di risparmiare tempo. Possono essere di vari tipi: Primo, secondo livello. Più a basso livello, più sono veloci e meno sono capienti (questione di costi). Su una architettura multicore? Le cache di primo livello sono sempre personali per i core; dai secondi in poi si può condividere oppure continuare ad essere personali. Nella cache condivisa, è più grossa però i controller sono più complessi. Avere cache separate: i controller sono più semplici ma ci sono problemi di coerenza. (aggiornamento delle informazioni, fetch problematiche a causa delle mancate sincronizzazioni fra caches)

--> Disco rigido funziona con le coordinate a cilindro -> i dischi magnetici sovrapposti, la testina va nella coordinata corrispondente della distanza puntina-perno. Questa visione a 3 coordinate può essere linearizzata. Astrazione che può aiutare. I tempi di accesso ad una informazione dipendono da tanti fattori
    -> Tempo di posizionamento (Dipende da un movimento fisico)
    -> Cilindro: collezione di tracce. C'è una traccia su ogni disco -> distinta per la propria distanza dal perno e dal numero/altezza del disco stesso.

---

## Input/Output

Controller dei dischi: un mini calcolatore dentro il calcolatore (ha una sua cpu e interfaccia) e ha il compito di pilotare gli N dischi e fare tutte quelle operazioni che servono --> è pilotato dal sistema operativo e dialoga con esso. Può utilizzare diversi sistemi di coordinate, e il tipo di istruzioni e le istruzioni offerte dal controller sono  più semplici rispetto a quello che il controller fa: non si preoccupa dei movimenti della testina e la geometria del disco. STANDARD SATA, o PATA (IDE) o SCSI. Sono standardizzati. Si sposta la logica dei controlli nei controllers. Si abbattono anche i costi della produzione dei dischi.

Però l'interfaccia Controller<->Sistema operativo non è standard. Questo gap viene gestito dai drivers. E' un pezzo di software, una collezione di metodi che traduce/mappa le interfacce in questo caso, permettendo a SO e controller dei dischi di comunicare. Spesso il driver viene scritto dagli stessi produttore del controller dei dischi. E si può sempre scrivere conoscendo le specifiche di controller e SO. 